#!/usr/bin/env python3
"""
ã»ã£ã‘ ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆå–å¾—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
hook_performance.json ã®æœªå–å¾—ã‚¨ãƒ³ãƒˆãƒªã«å¯¾ã—ã¦X APIã§ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã‚’ä¸€æ‹¬å–å¾—ã—è¨ºæ–­ã™ã‚‹ã€‚
"""

import os
import sys
import json
import re
import argparse
import subprocess
import shutil
from pathlib import Path
from datetime import datetime, timezone, timedelta
from typing import Optional
from x_api_client import XApiClient

sys.path.insert(0, str(Path(__file__).resolve().parent.parent))
from notifications.discord_notifier import DiscordNotifier

try:
    import tweepy
except ImportError:
    print("tweepyãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
    print("pip install tweepy python-dotenv")
    sys.exit(1)

try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

SCRIPT_DIR = Path(__file__).parent
HOOK_PERF_FILE = SCRIPT_DIR.parent / "hook_performance.json"


def diagnose(likes: int, retweets: int, impressions: int = 0) -> str:
    # --- ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼å°‘æ•°æœŸï¼ˆ~æ•°ç™¾ï¼‰: ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³åŸºæº– ---
    # ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãƒªãƒ¼ãƒã‚’ä¸»æŒ‡æ¨™ã¨ã™ã‚‹
    if impressions >= 50:
        return "SCALE"   # ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³3æœ¬ã™ãä½œã‚‹
    elif impressions >= 30:
        return "GOOD"    # ãã®ã‚«ãƒ†ã‚´ãƒªç¶™ç¶š
    elif impressions >= 10:
        return "OK"      # åˆ¥ã‚¢ãƒ³ã‚°ãƒ«ã§1å›å†æŒ‘æˆ¦
    else:
        return "DROP"    # åˆ¥ã‚«ãƒ†ã‚´ãƒªã«åˆ‡ã‚Šæ›¿ãˆ
    # --- ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼å¢—åŠ å¾Œï¼ˆæ•°ç™¾ã€œï¼‰: ã„ã„ã­+RTåŸºæº–ã«æˆ»ã™ ---
    # total = likes + retweets
    # if total >= 50:   return "SCALE"
    # elif total >= 10: return "GOOD"
    # elif total >= 3:  return "OK"
    # else:             return "DROP"


def load_perf_data() -> dict:
    if not HOOK_PERF_FILE.exists():
        return {"version": "1.0", "posts": []}
    with open(HOOK_PERF_FILE, 'r', encoding='utf-8') as f:
        return json.load(f)


def save_perf_data(data: dict) -> None:
    with open(HOOK_PERF_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def get_pending_posts(data: dict, threshold_hours: int) -> list:
    """ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæœªå–å¾—ã‹ã¤é–¾å€¤æ™‚é–“çµŒéæ¸ˆã¿ã®æŠ•ç¨¿ã‚’è¿”ã™"""
    now = datetime.now(timezone.utc)
    pending = []
    for post in data["posts"]:
        if post.get("engagementFetchedAt") is not None:
            continue
        posted_at_str = post.get("postedAt")
        if not posted_at_str:
            continue
        # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³æƒ…å ±ãŒã‚ã‚‹å ´åˆã¨ãªã„å ´åˆã‚’ä¸¡å¯¾å¿œ
        try:
            posted_at = datetime.fromisoformat(posted_at_str)
            if posted_at.tzinfo is None:
                posted_at = posted_at.replace(tzinfo=timezone(timedelta(hours=9)))  # JST
        except ValueError:
            continue
        elapsed_hours = (now - posted_at).total_seconds() / 3600
        if elapsed_hours >= threshold_hours:
            pending.append(post)
    return pending


def fetch_engagement(api_client: XApiClient, posts: list) -> list:
    """ãƒãƒƒãƒã§ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã‚’å–å¾—ã—ã¦ posts ã‚’æ›´æ–°ã—ã¦è¿”ã™"""
    tweet_ids = [p["tweet_id"] for p in posts]
    now_str = datetime.now().astimezone().strftime('%Y-%m-%dT%H:%M:%S')

    # æœ€å¤§100ä»¶ãšã¤ãƒãƒƒãƒå‡¦ç†
    updated = []
    for batch_start in range(0, len(tweet_ids), 100):
        batch_ids = tweet_ids[batch_start:batch_start + 100]
        batch_posts = posts[batch_start:batch_start + 100]

        try:
            response = api_client.get_tweets_public_metrics(batch_ids)
        except tweepy.TweepyException as e:
            print(f"[ERROR] APIå‘¼ã³å‡ºã—å¤±æ•—: {e}")
            updated.extend(batch_posts)
            continue

        if not response.data:
            print(f"[WARN] ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ãªã—ï¼ˆbatch {batch_start}ï¼‰")
            updated.extend(batch_posts)
            continue

        # tweet_id â†’ data ã®ãƒãƒƒãƒ—ã‚’ä½œæˆ
        tweet_map = {str(t.id): t for t in response.data}

        for post in batch_posts:
            tid = post["tweet_id"]
            tweet = tweet_map.get(tid)
            if not tweet:
                print(f"[WARN] tweet_id={tid} ãŒè¦‹ã¤ã‹ã‚‰ãªã„")
                updated.append(post)
                continue

            pub = tweet.public_metrics or {}

            post["likes"] = pub.get("like_count")
            post["retweets"] = pub.get("retweet_count")
            post["replies"] = pub.get("reply_count")
            post["quotes"] = pub.get("quote_count")
            post["bookmarks"] = pub.get("bookmark_count")
            post["impressions"] = None  # Free ãƒ—ãƒ©ãƒ³ã§ã¯å–å¾—ä¸å¯
            post["url_link_clicks"] = None
            post["user_profile_clicks"] = None
            post["engagementFetchedAt"] = now_str

            likes = post["likes"] or 0
            retweets = post["retweets"] or 0
            post["diagnosis"] = diagnose(likes, retweets, impressions=0)

            print(
                f"[å–å¾—] {post['hookCategory']} | "
                f"likes={post['likes']} RT={post['retweets']} "
                f"imp={post['impressions']} â†’ {post['diagnosis']}"
            )
            updated.append(post)

    return updated


def get_or_fetch_user_id(data: dict, api_client: XApiClient) -> str:
    """my_user_id ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—ã€ãªã‘ã‚Œã°APIã§å–å¾—ã—ã¦ä¿å­˜"""
    if data.get("my_user_id"):
        return data["my_user_id"]
    response = api_client.get_me()
    user_id = str(response.data.id)
    data["my_user_id"] = user_id
    print(f"[sync] user_id å–å¾—ãƒ»ä¿å­˜: {user_id}")
    return user_id


def sync_timeline(api_client: XApiClient, data: dict) -> int:
    """ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‚’å–å¾—ã—ã¦ hook_performance.json ã« upsertã€‚è¿”ã‚Šå€¤ã¯è¿½åŠ +æ›´æ–°ä»¶æ•°ã€‚"""
    user_id = get_or_fetch_user_id(data, api_client)
    since_id = data.get("last_since_id")

    tweets = api_client.get_user_tweets(user_id, max_results=100, since_id=since_id)
    if not tweets:
        print(f"[sync] æ–°è¦ãƒ„ã‚¤ãƒ¼ãƒˆãªã—ï¼ˆsince_id={since_id}ï¼‰")
        return 0

    existing_ids = {p["tweet_id"] for p in data["posts"]}
    now_str = datetime.now().astimezone().strftime('%Y-%m-%dT%H:%M:%S')
    max_id: Optional[str] = since_id
    added = 0
    updated = 0

    for tweet in tweets:
        tid = str(tweet.id)
        is_new = tid not in existing_ids

        if max_id is None or int(tid) > int(max_id):
            max_id = tid

        pub = tweet.public_metrics or {}
        non_pub = tweet.non_public_metrics or {}
        ref_types = {r["type"] for r in (tweet.referenced_tweets or [])}
        if tweet.in_reply_to_user_id is not None:
            tweet_type = "reply"
        elif "quoted" in ref_types:
            tweet_type = "quote"
        else:
            tweet_type = "post"
        likes = pub.get("like_count")
        retweets = pub.get("retweet_count")
        impressions = (non_pub.get("impression_count") or 0)
        diagnosis = diagnose(likes or 0, retweets or 0, impressions=impressions)

        if tweet.created_at:
            posted_at = tweet.created_at.astimezone(
                timezone(timedelta(hours=9))
            ).strftime('%Y-%m-%dT%H:%M:%S')
        else:
            posted_at = now_str

        if is_new:
            hook_category = "ãƒªãƒ—ãƒ©ã‚¤" if tweet_type == "reply" else "æœªåˆ†é¡"
            data["posts"].append({
                "tweet_id": tid,
                "text": tweet.text,
                "hookCategory": hook_category,
                "tweet_type": tweet_type,
                "postedAt": posted_at,
                "engagementFetchedAt": now_str,
                "likes": likes,
                "retweets": retweets,
                "replies": pub.get("reply_count"),
                "quotes": pub.get("quote_count"),
                "bookmarks": pub.get("bookmark_count"),
                "impressions": non_pub.get("impression_count"),
                "engagements": non_pub.get("engagements"),
                "url_link_clicks": non_pub.get("url_link_clicks"),
                "user_profile_clicks": non_pub.get("user_profile_clicks"),
                "diagnosis": diagnosis,
            })
            existing_ids.add(tid)
            added += 1
        else:
            for post in data["posts"]:
                if post["tweet_id"] == tid:
                    post["likes"] = likes
                    post["retweets"] = retweets
                    post["replies"] = pub.get("reply_count")
                    post["quotes"] = pub.get("quote_count")
                    post["bookmarks"] = pub.get("bookmark_count")
                    post["impressions"] = non_pub.get("impression_count")
                    post["engagements"] = non_pub.get("engagements")
                    post["url_link_clicks"] = non_pub.get("url_link_clicks")
                    post["user_profile_clicks"] = non_pub.get("user_profile_clicks")
                    post["tweet_type"] = tweet_type
                    post["engagementFetchedAt"] = now_str
                    post["diagnosis"] = diagnosis
                    break
            updated += 1

        imp = non_pub.get("impression_count")
        label = "æ–°è¦" if is_new else "æ›´æ–°"
        print(
            f"[sync] {label} {tweet_type} | "
            f"likes={likes} RT={retweets} imp={imp} | "
            f"{tweet.text[:30]}..."
        )

    data["last_since_id"] = max_id
    print(f"[sync] å®Œäº†: æ–°è¦{added}ä»¶ / æ›´æ–°{updated}ä»¶ / last_since_id={max_id}")
    return added + updated


VALID_HOOK_CATEGORIES = ["çŒ«å†™çœŸ", "é‹­ã„ä¸€è¨€", "æ—¥å¸¸è¦³å¯Ÿ", "è„±åŠ›ç³»", "æ™‚äº‹ãƒã‚¿", "ãŸã¾ã«æœ‰ç›Š", "çŒ«Meme", "çŒ«vsäººé–“", "ã‚·ãƒ¥ãƒ¼ãƒ«çŒ«"]

CATEGORIZE_SYSTEM_PROMPT = """ã‚ãªãŸã¯ãƒ›ãƒƒã‚±ï¼ˆèŒ¶ãƒˆãƒ©çŒ«AIã‚¢ã‚«ã‚¦ãƒ³ãƒˆï¼‰ã®æŠ•ç¨¿åˆ†æã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä¸ãˆã‚‰ã‚ŒãŸæŠ•ç¨¿ãƒ†ã‚­ã‚¹ãƒˆã‚’ä»¥ä¸‹ã®ã‚«ãƒ†ã‚´ãƒªã®ã©ã‚Œã‹1ã¤ã«åˆ†é¡ã—ã¦ãã ã•ã„ã€‚

ã‚«ãƒ†ã‚´ãƒªä¸€è¦§:
- çŒ«å†™çœŸ: ç”»åƒä»˜ãã€çŒ«ã®æ§˜å­ã‚’è¦‹ã›ã‚‹æŠ•ç¨¿ï¼ˆãƒªã‚¢ãƒ«çŒ«å†™çœŸï¼‰
- é‹­ã„ä¸€è¨€: äººé–“vsçŒ«ã®å“²å­¦çš„è¦³å¯Ÿã€ç¤¾ä¼šã¸ã®çš®è‚‰ãƒ»æ°—ã¥ã
- æ—¥å¸¸è¦³å¯Ÿ: é£¼ã„ä¸»ã‚„æ—¥å¸¸ã®å‡ºæ¥äº‹ã‚’æ·¡ã€…ã¨æå†™
- è„±åŠ›ç³»: ã‚„ã‚‹æ°—ã®ãªã•ãƒ»çœ ã„ãƒ»ã©ã†ã§ã‚‚ã„ã„ç³»
- æ™‚äº‹ãƒã‚¿: æ™‚äº‹ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰ã¸ã®çŒ«ç›®ç·šã‚³ãƒ¡ãƒ³ãƒˆ
- ãŸã¾ã«æœ‰ç›Š: å®Ÿç”¨çš„ãƒ»æœ‰ç›Šãªæƒ…å ±ã‚’å«ã‚€
- çŒ«Meme: å…±æ„Ÿãƒ»ã‚ã‚‹ã‚ã‚‹ç³»ã®Memeç”»åƒä»˜ãæŠ•ç¨¿
- çŒ«vsäººé–“: çŒ«ã¨äººé–“ã®ç”Ÿæ´»ã‚’å¯¾æ¯”ã™ã‚‹ç”»åƒä»˜ãæŠ•ç¨¿
- ã‚·ãƒ¥ãƒ¼ãƒ«çŒ«: çŒ«ãŒäººé–“ã®è¡Œå‹•ã‚’ã—ã¦ã„ã‚‹ã‚·ãƒ¥ãƒ¼ãƒ«ç”»åƒä»˜ãæŠ•ç¨¿

ã‚«ãƒ†ã‚´ãƒªåã®ã¿ã‚’1å˜èªã§è¿”ã—ã¦ãã ã•ã„ã€‚ä½™è¨ˆãªèª¬æ˜ã¯ä¸è¦ã§ã™ã€‚"""


def _call_claude(prompt: str, timeout: int = 30) -> Optional[str]:
    claude_cmd = shutil.which("claude")
    if not claude_cmd:
        fallback = "/home/sekiz/.nvm/versions/node/v24.13.0/bin/claude"
        if Path(fallback).exists():
            claude_cmd = fallback
        else:
            return None
    # CLAUDECODE ã‚’é™¤ã„ãŸç’°å¢ƒå¤‰æ•°ï¼ˆãƒã‚¹ãƒˆèµ·å‹•ãƒ–ãƒ­ãƒƒã‚¯ã‚’å›é¿ï¼‰
    env = {k: v for k, v in os.environ.items() if k != "CLAUDECODE"}
    try:
        result = subprocess.run(
            [claude_cmd, "-p", prompt],
            capture_output=True,
            text=True,
            timeout=timeout,
            env=env,
        )
        if result.returncode == 0:
            return result.stdout.strip()
        return None
    except (subprocess.TimeoutExpired, FileNotFoundError):
        return None


def categorize_unknown_posts(data: dict) -> int:
    """hookCategory='æœªåˆ†é¡' ã®æŠ•ç¨¿ã‚’ Claude ã§è‡ªå‹•åˆ†é¡ã™ã‚‹ã€‚è¿”ã‚Šå€¤ã¯æ›´æ–°ä»¶æ•°ã€‚"""
    unknown = [p for p in data["posts"] if p.get("hookCategory") == "æœªåˆ†é¡"]
    if not unknown:
        return 0

    print(f"[categorize] æœªåˆ†é¡: {len(unknown)}ä»¶ â†’ Claude ã§åˆ†é¡ã—ã¾ã™", flush=True)
    updated = 0

    for post in unknown:
        prompt = f"""{CATEGORIZE_SYSTEM_PROMPT}

æŠ•ç¨¿ãƒ†ã‚­ã‚¹ãƒˆ:
{post['text']}"""

        result = _call_claude(prompt)
        if not result:
            print(f"  [SKIP] Claude å¿œç­”ãªã—: {post['text'][:30]}...", flush=True)
            continue

        # ã‚«ãƒ†ã‚´ãƒªåã‚’æ­£è¦åŒ–ï¼ˆä½™åˆ†ãªæ–‡å­—é™¤å»ï¼‰
        category = result.strip().strip("ã€Œã€'\"")
        if category not in VALID_HOOK_CATEGORIES:
            # éƒ¨åˆ†ä¸€è‡´ã§æ•‘æ¸ˆ
            matched = next((c for c in VALID_HOOK_CATEGORIES if c in category), None)
            if matched:
                category = matched
            else:
                print(f"  [SKIP] ä¸æ˜ã‚«ãƒ†ã‚´ãƒª '{category}': {post['text'][:30]}...", flush=True)
                continue

        post["hookCategory"] = category
        print(f"  [{category}] {post['text'][:50]}...", flush=True)
        updated += 1

    print(f"[categorize] å®Œäº†: {updated}/{len(unknown)}ä»¶ åˆ†é¡æ¸ˆã¿", flush=True)
    return updated


def print_recommend(data: dict) -> None:
    """ä»Šæ—¥ã®æŠ•ç¨¿ã‚«ãƒ†ã‚´ãƒªæ¨è–¦ã‚’è¡¨ç¤ºï¼ˆAPIã‚³ãƒ¼ãƒ«ãªã—ï¼‰"""
    from collections import defaultdict

    VALID_CATEGORIES = ["è„±åŠ›ç³»", "çŒ«å†™çœŸ", "é‹­ã„ä¸€è¨€", "æ—¥å¸¸è¦³å¯Ÿ", "æ™‚äº‹ãƒã‚¿", "ãŸã¾ã«æœ‰ç›Š", "çŒ«Meme", "çŒ«vsäººé–“", "ã‚·ãƒ¥ãƒ¼ãƒ«çŒ«", "æœªåˆ†é¡"]

    # è¨ºæ–­æ¸ˆã¿æŠ•ç¨¿ã‚’ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«åˆ†é¡ï¼ˆæŠ•ç¨¿æ—¥æ™‚é †ï¼‰
    categories: dict = defaultdict(list)
    for post in data["posts"]:
        if post.get("diagnosis"):
            cat = post.get("hookCategory", "æœªåˆ†é¡")
            categories[cat].append(post)

    # å„ã‚«ãƒ†ã‚´ãƒªã‚’æŠ•ç¨¿æ—¥æ™‚ã®æ–°ã—ã„é †ã«ã‚½ãƒ¼ãƒˆ
    for cat in categories:
        categories[cat].sort(key=lambda p: p.get("postedAt", ""), reverse=True)

    lines_priority = []
    lines_candidate = []
    lines_ng = []
    lines_unknown = []

    seen_cats = set(categories.keys())

    for cat, posts in categories.items():
        n = len(posts)
        avg = sum((p.get("likes") or 0) + (p.get("retweets") or 0) for p in posts) / n
        latest_diag = posts[0].get("diagnosis", "")
        second_diag = posts[1].get("diagnosis", "") if n >= 2 else ""

        if n >= 2 and latest_diag == "DROP" and second_diag == "DROP":
            lines_ng.append(f"NG:   {cat} [DROP x2] avg={avg:.0f} ({n}ä»¶) â† ä»Šæ—¥ã¯é¿ã‘ã‚‹")
        elif latest_diag in ("SCALE", "GOOD"):
            lines_priority.append(f"å„ªå…ˆ: {cat} [{latest_diag}] avg={avg:.0f} ({n}ä»¶)")
        elif latest_diag == "OK":
            lines_candidate.append(f"å€™è£œ: {cat} [OK] avg={avg:.0f} ({n}ä»¶)")
        else:
            # DROPã ãŒé€£ç¶šã§ã¯ãªã„
            lines_candidate.append(f"å€™è£œ: {cat} [DROP] avg={avg:.0f} ({n}ä»¶)")

    # ãƒ‡ãƒ¼ã‚¿ãªã—ã®ã‚«ãƒ†ã‚´ãƒª
    for cat in VALID_CATEGORIES:
        if cat not in seen_cats and cat != "æœªåˆ†é¡":
            lines_unknown.append(f"æœªçŸ¥: {cat} (ãƒ‡ãƒ¼ã‚¿ãªã— â†’ è©¦ã—ã¦ã‚‚OK)")

    print("\n=== ä»Šæ—¥ã®æŠ•ç¨¿ã‚«ãƒ†ã‚´ãƒªæ¨è–¦ ===")
    for line in lines_priority:
        print(line)
    for line in lines_candidate:
        print(line)
    for line in lines_ng:
        print(line)
    for line in lines_unknown:
        print(line)

    if not (lines_priority or lines_candidate or lines_ng or lines_unknown):
        print("ï¼ˆãƒ‡ãƒ¼ã‚¿ãªã— â€” ã¾ãšæŠ•ç¨¿ã—ã¦ã‚«ãƒ†ã‚´ãƒªãƒ‡ãƒ¼ã‚¿ã‚’è“„ç©ã—ã¦ãã ã•ã„ï¼‰")


def print_summary(data: dict) -> None:
    """ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹é›†è¨ˆã‚’è¡¨ç¤º"""
    fetched = [p for p in data["posts"] if p.get("engagementFetchedAt")]
    if not fetched:
        print("é›†è¨ˆå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãªã—ï¼ˆã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆå–å¾—æ¸ˆã¿ã®æŠ•ç¨¿ãŒã‚ã‚Šã¾ã›ã‚“ï¼‰")
        return

    from collections import defaultdict
    categories: dict = defaultdict(list)
    for post in fetched:
        categories[post.get("hookCategory", "æœªåˆ†é¡")].append(post)

    print("\n=== ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹é›†è¨ˆ ===")
    for cat, posts in sorted(categories.items()):
        n = len(posts)
        avg_likes = sum(p["likes"] or 0 for p in posts) / n
        avg_rt = sum(p["retweets"] or 0 for p in posts) / n
        avg_imp = sum(p["impressions"] or 0 for p in posts) / n

        diagnosis_counts: dict = defaultdict(int)
        for p in posts:
            if p.get("diagnosis"):
                diagnosis_counts[p["diagnosis"]] += 1

        diag_str = " / ".join(
            f"{k}: {v}ä»¶" for k, v in sorted(diagnosis_counts.items())
        )

        print(
            f"\n[{cat}] {n}ä»¶  "
            f"å¹³å‡ã„ã„ã­:{avg_likes:.1f} / å¹³å‡RT:{avg_rt:.1f} / å¹³å‡impressions:{avg_imp:.0f}"
        )
        if diag_str:
            print(f"  {diag_str}")


def build_quote_analysis_summary(data: dict) -> str:
    """hook_performance.json ã‹ã‚‰å¼•ç”¨ãƒ„ã‚¤ãƒ¼ãƒˆã®ã¿ã‚’ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«é›†è¨ˆ"""
    from collections import defaultdict, Counter
    quotes = [p for p in data["posts"]
              if p.get("engagementFetchedAt")
              and p.get("tweet_type") == "quote"
              and p.get("hookCategory") not in ("æœªåˆ†é¡",)]
    if not quotes:
        return "å¼•ç”¨ãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ãªã—"

    categories: dict = defaultdict(list)
    for post in quotes:
        categories[post.get("hookCategory", "æœªåˆ†é¡")].append(post)

    lines = [f"åˆ†æå¯¾è±¡: å¼•ç”¨ãƒ„ã‚¤ãƒ¼ãƒˆ {len(quotes)}ä»¶\n"]
    for cat, posts in sorted(categories.items(),
                              key=lambda x: -(sum(p.get("impressions") or 0 for p in x[1]) / len(x[1]))):
        n = len(posts)
        avg_imp = sum(p.get("impressions") or 0 for p in posts) / n
        avg_likes = sum(p.get("likes") or 0 for p in posts) / n
        diag = Counter(p.get("diagnosis", "DROP") for p in posts)
        recent = sorted(posts, key=lambda p: p.get("postedAt", ""), reverse=True)[:3]
        lines.append(f"ã€{cat}ã€‘{n}ä»¶ å¹³å‡imp={avg_imp:.0f} å¹³å‡ã„ã„ã­={avg_likes:.1f}")
        lines.append(f"  è¨ºæ–­: {dict(diag)}")
        for p in recent:
            lines.append(f"  - imp={p.get('impressions')} likes={p.get('likes')} ã€Œ{p['text'][:40]}ã€")
    return "\n".join(lines)


STRATEGY_FILE = SCRIPT_DIR.parent / "post_scheduler" / "strategy.json"
REPLY_LOG_FILE = SCRIPT_DIR.parent / "reply_system" / "reply_log.json"
REPLY_STRATEGY_FILE = SCRIPT_DIR.parent / "reply_system" / "reply_strategy.json"


def migrate_replies(data: dict) -> int:
    """hook_performance.json ã® hookCategory='ãƒªãƒ—ãƒ©ã‚¤' ã‚’ reply_log.json ã®ã‚«ãƒ†ã‚´ãƒªã§æ›´æ–°ã™ã‚‹"""
    if not REPLY_LOG_FILE.exists():
        print("[migrate] reply_log.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return 0

    with open(REPLY_LOG_FILE, 'r', encoding='utf-8') as f:
        reply_log = json.load(f)

    # reply_log ã®ãƒ†ã‚­ã‚¹ãƒˆ â†’ ã‚«ãƒ†ã‚´ãƒª ãƒãƒƒãƒ”ãƒ³ã‚°æ§‹ç¯‰
    text_to_category = {}
    for entry in reply_log:
        if entry.get("status") == "posted" and entry.get("reply_text") and entry.get("category"):
            text_to_category[entry["reply_text"].strip()] = entry["category"]

    updated = 0
    for post in data["posts"]:
        if post.get("hookCategory") != "ãƒªãƒ—ãƒ©ã‚¤":
            continue
        text = (post.get("text") or "").strip()
        # hook_performance ã®ãƒ†ã‚­ã‚¹ãƒˆã¯ "@username æœ¬æ–‡" å½¢å¼ãªã®ã§ã€æœ¬æ–‡éƒ¨åˆ†ã‚’æŠ½å‡º
        text_body = re.sub(r'^@\S+\s+', '', text)
        # å®Œå…¨ä¸€è‡´
        category = text_to_category.get(text_body)
        if not category:
            # reply_log ã®ãƒ†ã‚­ã‚¹ãƒˆãŒ post.text ã«å«ã¾ã‚Œã‚‹ã‹
            for reply_text, cat in text_to_category.items():
                if reply_text and reply_text in text:
                    category = cat
                    break
        if category:
            post["hookCategory"] = category
            post["tweet_type"] = "reply"
            print(f"  [migrate] {category}: {text[:40]}...")
            updated += 1

    print(f"[migrate] å®Œäº†: {updated}ä»¶ã®ãƒªãƒ—ãƒ©ã‚¤ã‚«ãƒ†ã‚´ãƒªã‚’æ›´æ–°")
    return updated


def build_analysis_summary(data: dict) -> str:
    """hook_performance.json ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã®åˆ†æã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆã™ã‚‹"""
    from collections import defaultdict
    fetched = [p for p in data["posts"] if p.get("engagementFetchedAt") and p.get("tweet_type") not in ("reply", "quote") and p.get("hookCategory") != "ãƒªãƒ—ãƒ©ã‚¤"]
    if not fetched:
        return "ãƒ‡ãƒ¼ã‚¿ãªã—"

    categories: dict = defaultdict(list)
    for post in fetched:
        categories[post.get("hookCategory", "æœªåˆ†é¡")].append(post)

    lines = [f"åˆ†æå¯¾è±¡: é€šå¸¸æŠ•ç¨¿ {len(fetched)}ä»¶ï¼ˆãƒªãƒ—ãƒ©ã‚¤ãƒ»å¼•ç”¨é™¤ãï¼‰\n"]
    for cat, posts in sorted(categories.items(), key=lambda x: -(sum(p.get("impressions") or 0 for p in x[1]) / len(x[1]))):
        n = len(posts)
        avg_imp = sum(p.get("impressions") or 0 for p in posts) / n
        avg_likes = sum(p.get("likes") or 0 for p in posts) / n
        from collections import Counter
        diag = Counter(p.get("diagnosis", "DROP") for p in posts)
        recent = sorted(posts, key=lambda p: p.get("postedAt", ""), reverse=True)[:3]
        lines.append(f"ã€{cat}ã€‘{n}ä»¶ å¹³å‡imp={avg_imp:.0f} å¹³å‡ã„ã„ã­={avg_likes:.1f}")
        lines.append(f"  è¨ºæ–­: {dict(diag)}")
        for p in recent:
            lines.append(f"  - imp={p.get('impressions')} likes={p.get('likes')} ã€Œ{p['text'][:40]}ã€")
    return "\n".join(lines)


def build_reply_analysis_summary(data: dict) -> str:
    """hook_performance.json ã‹ã‚‰ãƒªãƒ—ãƒ©ã‚¤ã®ã¿ã‚’ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«é›†è¨ˆ"""
    from collections import defaultdict, Counter
    replies = [p for p in data["posts"]
               if p.get("engagementFetchedAt")
               and p.get("tweet_type") == "reply"
               and p.get("hookCategory") not in ("ãƒªãƒ—ãƒ©ã‚¤", "æœªåˆ†é¡")]
    if not replies:
        return "ãƒªãƒ—ãƒ©ã‚¤ãƒ‡ãƒ¼ã‚¿ãªã—"

    categories: dict = defaultdict(list)
    for post in replies:
        categories[post.get("hookCategory", "æœªåˆ†é¡")].append(post)

    lines = [f"åˆ†æå¯¾è±¡: ãƒªãƒ—ãƒ©ã‚¤ {len(replies)}ä»¶\n"]
    for cat, posts in sorted(categories.items(),
                              key=lambda x: -(sum(p.get("impressions") or 0 for p in x[1]) / len(x[1]))):
        n = len(posts)
        avg_imp = sum(p.get("impressions") or 0 for p in posts) / n
        avg_likes = sum(p.get("likes") or 0 for p in posts) / n
        diag = Counter(p.get("diagnosis", "DROP") for p in posts)
        recent = sorted(posts, key=lambda p: p.get("postedAt", ""), reverse=True)[:3]
        lines.append(f"ã€{cat}ã€‘{n}ä»¶ å¹³å‡imp={avg_imp:.0f} å¹³å‡ã„ã„ã­={avg_likes:.1f}")
        lines.append(f"  è¨ºæ–­: {dict(diag)}")
        for p in recent:
            lines.append(f"  - imp={p.get('impressions')} likes={p.get('likes')} ã€Œ{p['text'][:40]}ã€")
    return "\n".join(lines)


def run_act_reply(data: dict) -> None:
    """ãƒªãƒ—ãƒ©ã‚¤ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã¦ reply_strategy.json ã‚’ç”Ÿæˆ"""
    summary = build_reply_analysis_summary(data)
    if summary == "ãƒªãƒ—ãƒ©ã‚¤ãƒ‡ãƒ¼ã‚¿ãªã—":
        print("[act-reply] ãƒªãƒ—ãƒ©ã‚¤ãƒ‡ãƒ¼ã‚¿ãªã—ã€‚ã‚¹ã‚­ãƒƒãƒ—", flush=True)
        return

    today = datetime.now().strftime("%Y-%m-%d")
    prompt = f"""ã‚ãªãŸã¯ãƒ›ãƒƒã‚±ï¼ˆèŒ¶ãƒˆãƒ©çŒ«AIã‚¢ã‚«ã‚¦ãƒ³ãƒˆï¼‰ã®ãƒªãƒ—ãƒ©ã‚¤æˆ¦ç•¥ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®ãƒªãƒ—ãƒ©ã‚¤ã®ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã€ä»Šå¾Œã®ãƒªãƒ—ãƒ©ã‚¤æˆ¦ç•¥ã‚’JSONã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

# æœ¬æ—¥ï¼ˆ{today}ï¼‰ã®ãƒªãƒ—ãƒ©ã‚¤ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿
{summary}

# å‡ºåŠ›å½¢å¼ï¼ˆJSONã®ã¿ãƒ»èª¬æ˜ä¸è¦ï¼‰
{{
  "preferred_categories": ["ã‚«ãƒ†ã‚´ãƒªå", ...],
  "avoid_categories": ["ã‚«ãƒ†ã‚´ãƒªå", ...],
  "guidance": "å…·ä½“çš„ãªãƒªãƒ—ãƒ©ã‚¤æŒ‡é‡ï¼ˆ100å­—ä»¥å†…ï¼‰",
  "reason": "æˆ¦ç•¥ã®æ ¹æ‹ ï¼ˆ50å­—ä»¥å†…ï¼‰",
  "updated_at": "{today}"
}}

ãƒ«ãƒ¼ãƒ«:
- preferred_categories: ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆãŒé«˜ã„ãƒªãƒ—ãƒ©ã‚¤ã‚«ãƒ†ã‚´ãƒªã‚’1ã€œ3å€‹
- avoid_categories: åå¿œãŒæ‚ªã„ã‚«ãƒ†ã‚´ãƒªï¼ˆãªã‘ã‚Œã°ç©ºé…åˆ—ï¼‰
- guidance: ãƒ›ãƒƒã‚±ã®ãƒšãƒ«ã‚½ãƒŠã«æ²¿ã£ãŸå…·ä½“çš„ãªãƒªãƒ—ãƒ©ã‚¤ã®æ–¹å‘æ€§
- ã‚«ãƒ†ã‚´ãƒªã¯æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã‚«ãƒ†ã‚´ãƒªï¼ˆçŒ«ç³»/è„±åŠ›ç³»/ãƒ¡ãƒ³ã‚¿ãƒ«ç³»/é£Ÿã¹ç‰©ç³» ç­‰ï¼‰"""

    print("[act-reply] Claude ã§ãƒªãƒ—ãƒ©ã‚¤æˆ¦ç•¥ã‚’ç”Ÿæˆä¸­...", flush=True)
    result = _call_claude(prompt, timeout=60)
    if not result:
        print("[act-reply] Claude å¿œç­”ãªã—ã€‚ã‚¹ã‚­ãƒƒãƒ—", flush=True)
        return

    import re
    json_match = re.search(r'\{[\s\S]*\}', result)
    if not json_match:
        print(f"[act-reply] JSON ãŒè¦‹ã¤ã‹ã‚‰ãªã„: {result[:100]}", flush=True)
        return

    try:
        strategy = json.loads(json_match.group())
    except json.JSONDecodeError as e:
        print(f"[act-reply] JSON ãƒ‘ãƒ¼ã‚¹å¤±æ•—: {e}", flush=True)
        return

    REPLY_STRATEGY_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(REPLY_STRATEGY_FILE, "w", encoding="utf-8") as f:
        json.dump(strategy, f, ensure_ascii=False, indent=2)

    print(f"[act-reply] ãƒªãƒ—ãƒ©ã‚¤æˆ¦ç•¥ã‚’ä¿å­˜: {REPLY_STRATEGY_FILE}", flush=True)
    print(f"  å„ªå…ˆ: {strategy.get('preferred_categories')}", flush=True)
    print(f"  å›é¿: {strategy.get('avoid_categories')}", flush=True)
    print(f"  æŒ‡é‡: {strategy.get('guidance')}", flush=True)


def run_act(data: dict) -> None:
    """åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’ Claude ã«æ¸¡ã—ã¦æˆ¦ç•¥ã‚’ç”Ÿæˆã— strategy.json ã«ä¿å­˜ã™ã‚‹"""
    summary = build_analysis_summary(data)
    today = datetime.now().strftime("%Y-%m-%d")

    prompt = f"""ã‚ãªãŸã¯ãƒ›ãƒƒã‚±ï¼ˆèŒ¶ãƒˆãƒ©çŒ«AIã‚¢ã‚«ã‚¦ãƒ³ãƒˆï¼‰ã®é‹ç”¨æˆ¦ç•¥ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã€æ˜æ—¥ä»¥é™ã®æŠ•ç¨¿æˆ¦ç•¥ã‚’JSONã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

# æœ¬æ—¥ï¼ˆ{today}ï¼‰ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿
{summary}

# å‡ºåŠ›å½¢å¼ï¼ˆJSONã®ã¿ãƒ»èª¬æ˜ä¸è¦ï¼‰
{{
  "preferred_categories": ["ã‚«ãƒ†ã‚´ãƒªå", ...],
  "avoid_categories": ["ã‚«ãƒ†ã‚´ãƒªå", ...],
  "guidance": "å…·ä½“çš„ãªæŠ•ç¨¿æŒ‡é‡ï¼ˆ100å­—ä»¥å†…ï¼‰",
  "reason": "æˆ¦ç•¥ã®æ ¹æ‹ ï¼ˆ50å­—ä»¥å†…ï¼‰",
  "updated_at": "{today}"
}}

ãƒ«ãƒ¼ãƒ«:
- preferred_categories: ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ãƒ»ã„ã„ã­ãŒé«˜ã„ã‚«ãƒ†ã‚´ãƒªã‚’1ã€œ3å€‹
- avoid_categories: DROP ãŒç¶šã„ã¦ã„ã‚‹ã‚«ãƒ†ã‚´ãƒªï¼ˆãªã‘ã‚Œã°ç©ºé…åˆ—ï¼‰
- guidance: ãƒ›ãƒƒã‚±ã®ãƒšãƒ«ã‚½ãƒŠã«æ²¿ã£ãŸå…·ä½“çš„ãªæŠ•ç¨¿ã®æ–¹å‘æ€§
- ã‚«ãƒ†ã‚´ãƒªã¯ è„±åŠ›ç³»/çŒ«å†™çœŸ/é‹­ã„ä¸€è¨€/æ—¥å¸¸è¦³å¯Ÿ/æ™‚äº‹ãƒã‚¿/ãŸã¾ã«æœ‰ç›Š/çŒ«Meme/çŒ«vsäººé–“/ã‚·ãƒ¥ãƒ¼ãƒ«çŒ« ã‹ã‚‰é¸ã¶"""

    print("[act] Claude ã§æˆ¦ç•¥ã‚’ç”Ÿæˆä¸­...", flush=True)
    result = _call_claude(prompt, timeout=60)
    if not result:
        print("[act] Claude å¿œç­”ãªã—ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™", flush=True)
        return

    # JSON æŠ½å‡ºï¼ˆãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å¯¾å¿œï¼‰
    import re
    json_match = re.search(r'\{[\s\S]*\}', result)
    if not json_match:
        print(f"[act] JSON ãŒè¦‹ã¤ã‹ã‚‰ãªã„: {result[:100]}", flush=True)
        return

    try:
        strategy = json.loads(json_match.group())
    except json.JSONDecodeError as e:
        print(f"[act] JSON ãƒ‘ãƒ¼ã‚¹å¤±æ•—: {e}", flush=True)
        return
    if not isinstance(strategy, dict):
        print(f"[act] JSON ãŒ dict ã§ãªã„: {type(strategy).__name__}", flush=True)
        return

    # æ—¢å­˜ã®éLLMãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼ˆimage_probability ç­‰ï¼‰ã‚’ä¿æŒã—ã¦ãƒãƒ¼ã‚¸
    STRATEGY_FILE.parent.mkdir(parents=True, exist_ok=True)
    _PRESERVE_KEYS = ("max_image_posts_per_day",)
    if STRATEGY_FILE.exists():
        try:
            with open(STRATEGY_FILE, "r", encoding="utf-8") as f:
                existing = json.load(f)
            for k in _PRESERVE_KEYS:
                if k in existing and k not in strategy:
                    strategy[k] = existing[k]
        except (OSError, json.JSONDecodeError):
            pass
    with open(STRATEGY_FILE, "w", encoding="utf-8") as f:
        json.dump(strategy, f, ensure_ascii=False, indent=2)

    print(f"[act] æˆ¦ç•¥ã‚’ä¿å­˜: {STRATEGY_FILE}", flush=True)
    print(f"  å„ªå…ˆ: {strategy.get('preferred_categories')}", flush=True)
    print(f"  å›é¿: {strategy.get('avoid_categories')}", flush=True)
    print(f"  æŒ‡é‡: {strategy.get('guidance')}", flush=True)

    # Discord é€šçŸ¥
    try:
        preferred = " / ".join(strategy.get("preferred_categories") or [])
        avoid = " / ".join(strategy.get("avoid_categories") or []) or "ãªã—"
        guidance = strategy.get("guidance", "")
        reason = strategy.get("reason", "")
        lines = [
            "**ğŸ“Š ãƒ›ãƒƒã‚± æ—¥æ¬¡ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ & æˆ¦ç•¥ãƒ¬ãƒãƒ¼ãƒˆ**",
            f"`date` {today}",
            "",
            "**ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ï¼ˆé€šå¸¸æŠ•ç¨¿ï¼‰**",
        ]
        # ã‚«ãƒ†ã‚´ãƒªé›†è¨ˆã‚’è¿½åŠ 
        from collections import defaultdict
        posts = [p for p in data["posts"] if p.get("engagementFetchedAt") and p.get("tweet_type") not in ("reply", "quote") and p.get("hookCategory") not in ("ãƒªãƒ—ãƒ©ã‚¤", "æœªåˆ†é¡")]
        cats: dict = defaultdict(list)
        for p in posts:
            cats[p["hookCategory"]].append(p)
        for cat, ps in sorted(cats.items(), key=lambda x: -(sum(p.get("impressions") or 0 for p in x[1]) / len(x[1]))):
            avg_imp = sum(p.get("impressions") or 0 for p in ps) / len(ps)
            lines.append(f"- `{cat}`: å¹³å‡imp {avg_imp:.0f} ({len(ps)}ä»¶)")
        lines += [
            "",
            "**ğŸ“Œ æ˜æ—¥ã®æŠ•ç¨¿æˆ¦ç•¥**",
            f"å„ªå…ˆ: `{preferred}`",
            f"å›é¿: `{avoid}`",
            f"æŒ‡é‡: {guidance}",
            f"æ ¹æ‹ : {reason}",
        ]
        # å¼•ç”¨ãƒ„ã‚¤ãƒ¼ãƒˆã‚»ã‚¯ã‚·ãƒ§ãƒ³
        quote_posts = [p for p in data["posts"] if p.get("engagementFetchedAt") and p.get("tweet_type") == "quote" and p.get("hookCategory") not in ("æœªåˆ†é¡",)]
        if quote_posts:
            quote_cats: dict = defaultdict(list)
            for p in quote_posts:
                quote_cats[p["hookCategory"]].append(p)
            lines += ["", "**ğŸ” å¼•ç”¨ãƒ„ã‚¤ãƒ¼ãƒˆ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹**"]
            for cat, ps in sorted(quote_cats.items(), key=lambda x: -(sum(p.get("impressions") or 0 for p in x[1]) / len(x[1]))):
                avg_imp = sum(p.get("impressions") or 0 for p in ps) / len(ps)
                avg_likes = sum(p.get("likes") or 0 for p in ps) / len(ps)
                lines.append(f"- `{cat}`: å¹³å‡imp {avg_imp:.0f} / å¹³å‡ã„ã„ã­ {avg_likes:.1f} ({len(ps)}ä»¶)")
        # ãƒªãƒ—ãƒ©ã‚¤æˆ¦ç•¥ã‚‚ã‚ã‚Œã°è¿½åŠ 
        if REPLY_STRATEGY_FILE.exists():
            try:
                with open(REPLY_STRATEGY_FILE, 'r', encoding='utf-8') as f:
                    rs = json.load(f)
                r_preferred = " / ".join(rs.get("preferred_categories") or [])
                r_avoid = " / ".join(rs.get("avoid_categories") or []) or "ãªã—"
                r_guidance = rs.get("guidance", "")
                lines += [
                    "",
                    "**ğŸ’¬ ãƒªãƒ—ãƒ©ã‚¤æˆ¦ç•¥**",
                ]
                # ãƒªãƒ—ãƒ©ã‚¤ã®ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³
                reply_posts = [p for p in data["posts"] if p.get("engagementFetchedAt") and p.get("tweet_type") == "reply" and p.get("hookCategory") not in ("ãƒªãƒ—ãƒ©ã‚¤", "æœªåˆ†é¡")]
                reply_cats: dict = defaultdict(list)
                for p in reply_posts:
                    reply_cats[p["hookCategory"]].append(p)
                if reply_cats:
                    lines.append("**ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ï¼ˆãƒªãƒ—ãƒ©ã‚¤ï¼‰**")
                    for cat, ps in sorted(reply_cats.items(), key=lambda x: -(sum(p.get("impressions") or 0 for p in x[1]) / len(x[1]))):
                        avg_imp = sum(p.get("impressions") or 0 for p in ps) / len(ps)
                        lines.append(f"- `{cat}`: å¹³å‡imp {avg_imp:.0f} ({len(ps)}ä»¶)")
                    lines.append("")
                lines += [
                    f"å„ªå…ˆ: `{r_preferred}`",
                    f"å›é¿: `{r_avoid}`",
                    f"æŒ‡é‡: {r_guidance}",
                ]
            except Exception:
                pass
        message = "\n".join(lines)
        notifier = DiscordNotifier.from_env("DISCORD_WEBHOOK_POST")
        result = notifier.send(message, username="ãƒ›ãƒƒã‚±æˆ¦ç•¥ãƒ¬ãƒãƒ¼ãƒˆ")
        if result.ok:
            print("[act] Discord é€šçŸ¥é€ä¿¡æˆåŠŸ", flush=True)
        else:
            print(f"[act] Discord é€šçŸ¥å¤±æ•—: {result.error}", flush=True)
    except Exception as e:
        print(f"[act] Discord é€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}", flush=True)


def main():
    parser = argparse.ArgumentParser(description='ã»ã£ã‘ ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆå–å¾—ãƒ»è¨ºæ–­')
    parser.add_argument(
        '--threshold-hours', type=int, default=24,
        help='æŠ•ç¨¿ã‹ã‚‰ã®çµŒéæ™‚é–“ï¼ˆæ™‚é–“ï¼‰ã®é–¾å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 24ï¼‰'
    )
    parser.add_argument(
        '--dry-run', action='store_true',
        help='APIã‚³ãƒ¼ãƒ«ãªã—ã§å¯¾è±¡ä¸€è¦§ã ã‘è¡¨ç¤º'
    )
    parser.add_argument(
        '--summary', action='store_true',
        help='ã‚«ãƒ†ã‚´ãƒªåˆ¥é›†è¨ˆã‚’è¡¨ç¤ºï¼ˆé€±æ¬¡ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨ï¼‰'
    )
    parser.add_argument(
        '--recommend', action='store_true',
        help='ä»Šæ—¥ã®æŠ•ç¨¿ã‚«ãƒ†ã‚´ãƒªæ¨è–¦ã‚’è¡¨ç¤ºï¼ˆAPIã‚³ãƒ¼ãƒ«ãªã—ãƒ»è‡ªå¾‹æŠ•ç¨¿ãƒã‚§ãƒƒã‚¯ç”¨ï¼‰'
    )
    parser.add_argument(
        '--sync', action='store_true',
        help='ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‚’å–å¾—ã—ã¦ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã‚’ä¸€æ‹¬syncï¼ˆé€šå¸¸æŠ•ç¨¿+ãƒªãƒ—ãƒ©ã‚¤ï¼‰'
    )
    parser.add_argument(
        '--act', action='store_true',
        help='ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã¦Claude ãŒæˆ¦ç•¥ã‚’ç”Ÿæˆã— strategy.json ã«ä¿å­˜'
    )
    parser.add_argument(
        '--migrate-replies', action='store_true',
        help='hook_performance.json ã®ãƒªãƒ—ãƒ©ã‚¤ã‚’ reply_log.json ã®ã‚«ãƒ†ã‚´ãƒªã§æ›´æ–°'
    )
    args = parser.parse_args()

    data = load_perf_data()

    if args.migrate_replies:
        migrate_replies(data)
        save_perf_data(data)
        return

    if args.recommend:
        print_recommend(data)
        return

    if args.sync:
        try:
            api_client = XApiClient(require_user_auth=True)
        except ValueError as e:
            print(f"[ERROR] {e}")
            sys.exit(1)
        sync_timeline(api_client, data)
        categorize_unknown_posts(data)
        save_perf_data(data)
        if args.act:
            run_act_reply(data)
            run_act(data)
        return

    if args.act:
        run_act_reply(data)
        run_act(data)
        return

    if args.summary:
        print_summary(data)
        return

    pending = get_pending_posts(data, args.threshold_hours)

    if not pending:
        print(f"å¯¾è±¡æŠ•ç¨¿ãªã—ï¼ˆé–¾å€¤: {args.threshold_hours}æ™‚é–“, æœªå–å¾—æŠ•ç¨¿æ•°: 0ï¼‰")
        return

    print(f"å¯¾è±¡: {len(pending)}ä»¶ï¼ˆé–¾å€¤: {args.threshold_hours}æ™‚é–“çµŒéæ¸ˆã¿ï¼‰")
    for p in pending:
        print(f"  - [{p['hookCategory']}] {p['postedAt']} | {p['text'][:30]}...")

    if args.dry_run:
        print("\n[dry-run] APIã‚³ãƒ¼ãƒ«ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        return

    try:
        api_client = XApiClient(require_bearer=True)
    except ValueError as e:
        print(f"[ERROR] {e}")
        sys.exit(1)

    # pending posts ã‚’ data["posts"] å†…ã®åŒä¸€ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå‚ç…§ã§æ›´æ–°
    fetch_engagement(api_client, pending)
    save_perf_data(data)
    print(f"\n[å®Œäº†] {len(pending)}ä»¶ã‚’æ›´æ–°ã—ã¾ã—ãŸ â†’ {HOOK_PERF_FILE}")


if __name__ == "__main__":
    main()
